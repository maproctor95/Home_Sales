# Home_Sales
## Overview of Project

### Instructions

* A Spark DataFrame is created from the dataset. (5 points)
* A temporary table of the original DataFrame is created. (10 points)
* A query is written that returns the average price, rounded to two decimal places, for a four-bedroom house that was sold in each year. (5 points)
* A query is written that returns the average price, rounded to two decimal places, of a home that has three bedrooms and three bathrooms. (5 points)
* A query is written that returns the average price of a home with three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet for each year built rounded to two decimal places. (5 points)
* A query is written that returns the view rating for the average price for homes that are greater than or equal to $350,000, rounded to two decimal places. (The output shows the run time for this query.) (10 points)
* A cache of the temporary "home_sales" table is created and validated. (10 points)
* The query from step 6 is run on the cached temporary table, and the run time is computed. (10 points)
* A partition of the home sales dataset by the "date_built" field is created, and the formatted parquet data is read. (10 points)
* A temporary table of the parquet data is created. (10 points)
* The query from step 6 is run on the parquet temporary table, and the run time is computed. (10 points)
* The "home_sales" temporary table is uncached and verified. (10 points)

### Purpose

I am utilizing jupyter notebook to analyze data from a dataset in order to get more detailed information about what the data shows.

## Analysis and Challenges

### Analysis of Outcomes Based on Goals

* During this assignment I was able to complete all the goals and objectives for the assignment. I used what I have learned in the class to trouble shoot the code as I went through with jupyter notebook and made sure I was fully understanding what each part of my code did through labeling, so when an error popped up I knew how to approach the problem.

### Challenges and Difficulties Encountered

* I think the biggest challenge for me was figuring out how to run such a big data source using jupyter notebook.
